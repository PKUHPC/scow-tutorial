{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial7: qwen2-72b-int4 推理\n",
    "\n",
    "本节旨在展示如何在 scow 平台上使用单机单卡运行 [Qwen2-72B-Instruct-GPTQ-Int4](https://huggingface.co/Qwen/Qwen2-72B-Instruct-GPTQ-Int4) 模型的推理任务。\n",
    "\n",
    "分以下几步来实现：\n",
    "1. 环境安装与应用创建\n",
    "2. 下载模型\n",
    "3. 模型推理\n",
    "\n",
    "\n",
    "Qwen 系列模型是由阿里巴巴开发的。Qwen 模型系列包括不同规模的模型，参数范围从 0.5 到 720 亿，适用于各种应用场景，如文本生成、翻译、问答等。Qwen2-72B-Instruct-GPTQ-Int4 支持高达 131,072 个 token 的上下文长度，能够处理大量输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境安装与应用创建\n",
    "\n",
    "首先在data shell中创建conda环境:\n",
    "```bash\n",
    "conda create -n tutorial7 python=3.9\n",
    "conda activate tutorial7\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "pip install notebook jupyterlab accelerate numpy==1.26.4 matplotlib==3.8.4 optimum>1.13.2 auto-gptq>0.4.2 transformers>=4.32.0,<4.38.0 accelerate tiktoken einops transformers_stream_generator==0.0.4 scipy\n",
    "pip install --upgrade pyarrow\n",
    "```\n",
    "\n",
    "（ pytorch 版本需与 cuda 版本对应，请查看版本对应网站：https://pytorch.org/get-started/previous-versions ）\n",
    "（ 通过 nvidia-smi 命令可查看 cuda 版本）\n",
    "\n",
    "然后创建JupyterLab应用, `Conda环境名`请填写`tutorial7`, 硬件资源为1个GPU，建议使用 A100 40G 或者 V100 32G。创建应用后, 进入应用并打开本文件。\n",
    "\n",
    "CUDA Version: 12.1; Torch Version: 2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 下载模型\n",
    "\n",
    "在data shell 中执行，命令执行位置在当前文件所在的文件夹。\n",
    "\n",
    "```bash\n",
    "# 如果以下目录存在， 可以直接复制:\n",
    "cp -r /lustre/public/tutorial/models/models--Qwen--Qwen2-72B-Instruct-GPTQ-Int4/ ./\n",
    "\n",
    "# 否则请下载:\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "huggingface-cli download --resume-download Qwen/Qwen2-72B-Instruct-GPTQ-Int4 --local-dir models--Qwen--Qwen2-72B-Instruct-GPTQ-Int4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型推理\n",
    "\n",
    "[[参考链接]](https://huggingface.co/Qwen/Qwen2-72B-Instruct-GPTQ-Int4)\n",
    "\n",
    "运行下方代码进行模型推理:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 使用 GPU\n",
    "device = \"cuda\"\n",
    "\n",
    "# 模型路径\n",
    "# VAR_PLACEHOLDER\n",
    "model_path = \"models--Qwen--Qwen2-72B-Instruct-GPTQ-Int4\"\n",
    "\n",
    "# 加载模型和分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prompt\n",
    "prompt = \"什么是大语言模型\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# 生成回答\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "推理过程中使用 nvidia-smi 命令可以查看 GPU 运行情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 作者: 黎颖; 龙汀汀\n",
    ">\n",
    "> 联系方式: yingliclaire@pku.edu.cn;   l.tingting@pku.edu.cn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
