{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2-72B-Instruct 单机多卡\n",
    "\n",
    "> 作者: 黎颖; 龙汀汀\n",
    ">\n",
    "> 联系方式: yingliclaire@pku.edu.cn;   l.tingting@pku.edu.cn\n",
    "\n",
    "\n",
    "本节展示如何在 SCOW 平台上使用单机多卡跑通 Qwen2-72B-Instruct 推理。\n",
    "\n",
    "分以下几步来实现：\n",
    "1. 环境安装与应用创建\n",
    "2. 下载模型\n",
    "3. 模型推理\n",
    "\n",
    "Qwen 系列模型是由阿里巴巴开发的。Qwen 模型系列包括不同规模的模型，参数范围从 0.5 到 720 亿，适用于各种应用场景，如文本生成、翻译、问答等。Qwen2-72B-Instruct 支持高达 131,072 个 token 的上下文长度，能够处理大量输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境安装与应用创建\n",
    "\n",
    "首先在data shell中创建conda环境:\n",
    "```bash\n",
    "conda create -n tutorial8 python=3.9\n",
    "conda activate tutorial8\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "pip install --upgrade transformers huggingface_hub\n",
    "```\n",
    "\n",
    "然后创建JupyterLab应用, `Conda环境名`请填写`tutorial8`, 建议的硬件资源为2张A100。创建应用后, 进入应用并打开本文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 下载模型\n",
    "\n",
    "在data shell 中执行，命令执行位置在当前文件所在的文件夹。\n",
    "\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "huggingface-cli download --resume-download Qwen/Qwen2-72B-Instruct --local-dir models--Qwen--Qwen2-72B-Instruct\n",
    "```\n",
    "\n",
    "如果使用 SCOW， 可以从公用存储中加载模型，存储所有模型的路径为：\n",
    "\n",
    "/lustre/public/tutorial/models/models--Qwen--Qwen2-72B-Instruct\n",
    "\n",
    "从中找到本节对应的模型加载即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型推理\n",
    "\n",
    "[[参考链接]](https://huggingface.co/Qwen/Qwen2-72B-Instruct-GPTQ-Int4)\n",
    "\n",
    "创建JupyterLab交互应用，进行模型推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 使用 GPU\n",
    "device = \"cuda\"\n",
    "\n",
    "# 模型路径\n",
    "# VAR_PLACEHOLDER\n",
    "model_path = \"huggingface-cli download --resume-download Qwen/Qwen2-72B-Instruct --local-dir models--Qwen--Qwen2-72B-Instruct\"\n",
    "\n",
    "# 加载模型和分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prompt\n",
    "prompt = \"什么是大模型\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# 生成回答\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "推理过程中使用 nvidia-smi 命令可以查看 GPU 运行情况。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
